---
title: "Text Mining: Tidying, Sentiment Analysis, & Topic Modeling"
subtitle: RLadies- Columbus
author: "Katie Sasso"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_float: true
---

# LEFT OFF CHANGE TO MAKE CORRECT 
```{r setup, include=FALSE, echo= TRUE, results='hide', message=FALSE}
knitr::opts_chunk$set(echo=FALSE, results='hide', message=FALSE)
```

# Getting Started 

##Cleaing your text data
The examples we will cover today involve open source text datasets that have been mostly cleaned for us. We will just have to work on reformatting these data sets to get them "Tidy".

When working with our own data this may not always be the case though. Often text data has inconsistent punctuation, spelling error, acronyms, and multiple ways of referring to the same "thing". Let's do a bit of cleaning with an original text dataset I created. Keep in mind this is just a few of the steps you may have to take to clean your own data. 

INSERT ACTIVITY LOG EXAMPLE 

```{r}
#word = str_extract(word, "[[:alpha:]]+")
#extract strings that match the patten spcified, in this case, extracting all alphabetic characters regardless of capitalization. So Numbers, symbols (i.e.,"_" and "*" often used to flag italics or bold will be removed-- we don't want don't want "Angry" to appear as a diff word than it's italicized form "_Angry_")

#We use str_extract() here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with #underscores around them to indicate emphasis (like italics). The tokenizer treated these as words, but we don’t #want to count “_any_” separately from “any”
```
## Some additional cleaning reasources to check out
Several helpful resources are available:

 * Regular expressions are often used to specify "patterns" in certain functions. [This regex cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) provides some helpful  string patterns.
 * This [regex matcher website](https://regex101.com/) is the most helpful free resource I've found for checking and testing that your regular expression is _actually_ specifying the string you think it's specifying. 

```{r}
?grepl
?str_extract
?str_detect
```


##The Tidy Text Format
A table with one-token-per-row, where a token is just a meaningful "unit" of text (e.g., word, sentence, or paragraph). This format often makes the data easier to manipulate and visualize using standard  "tidy" tools.

Different from how text is typically stored in many analytic text mining approaches, which often work on a "corpus" and/or a "document-term matrix". However we will see that transfer among the types is easy if our data is first "tidy".

Let's try it ourselves.

```{r}
#install these packages if you don't already have them
# install.packages("tidytext")
# install.packages("janeaustenr")
# install.packages("plyr")
# install.packages("dplyr")
# install.packages("stringr")
# install.packages("magrittr")


library(janeaustenr)
library(plyr)
library(dplyr)
library(stringr)
library(magrittr)

#browse the related vignette or others
vignette("tidytext",package="tidytext")
?austen_books

original_books <- austen_books() %>%
  group_by(book) %>% #grouping df by "book"
  mutate(linenumber = row_number(), #adding row numbers as a new variable
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",  
                                                 ignore_case = TRUE)))) %>%
  ungroup()
# the above is creating a new chapter variable by searching the "text" character variable for the string "chapter" and Roman Numeral characters (as specified in regex language here "[\\divxlc]")

head(original_books)
tail(original_books)
slice(original_books, 1:500) %>% View

#restructuring as a one-token-per-row format. unnest() does that for us
library(tidytext)

tidy_books <- original_books %>%
  unnest_tokens(word, text)
#The default tokenizing is for words, but other options include characters, ngrams, sentences, lines, paragraphs, or separation around a regex pattern.

data("stop_words")
head(stop_words)

cleaned_books <- tidy_books %>%
  anti_join(stop_words)

```

##Some Descriptives once Tidy

```{r}
#most common words in all the books?
cleaned_books %>%
  count(word, sort = TRUE)

#Examining and Visualizing most common words across all books and by book: 

cleaned_books_2 <- cleaned_books  %>% 
  mutate(word = str_extract(word, "[[:alpha:]]+")) %>%
  count(word,sort = TRUE) %>%  #counting the number of occurrences by group (word in this case)
  left_join(cleaned_books,.) %>% 
  rename(n_allbooks = n)  %>% 
  group_by(book,word) %>% 
  mutate(n_bybook = n()) %>% 
  arrange(book, desc(n_bybook)) %>% 
  ungroup()

#Chart of top 5 words across all books
library(ggplot2)

#doing all in one tidy flow
cleaned_books_2 %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

#using new var we created above
#no scientifict notation
options(scipen = 10000)
cleaned_books_2 %>% 
  filter(n_allbooks > 600, !duplicated(word)) %>% 
  mutate(word = reorder(word, n_allbooks)) %>% 
  ggplot(aes(word, n_allbooks)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

#What about the top 5 words only in each book? 
cleaned_books_2 %>% 
  group_by(book) %>% 
  filter(!duplicated(word)) %>% 
  arrange(desc(n_bybook)) %>% 
  top_n(3, n_bybook) %>% 
  ungroup %>% 
  arrange(book, n_bybook) %>% 
  ggplot(aes(word, n_bybook)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~book)

#other ways to visualize?? 
library(tidyr)

word_freqs <- cleaned_books %>% 
  mutate(word = str_extract(word, "[[:alpha:]]+")) %>% #Note if I would've added [a-z'] it would've been case sensitive! 
  count(book,word) %>% 
  group_by(book)  %>% 
  mutate(proportion = n/sum(n),
         n_bybook = n) %>% 
  select(-n) %>% 
  spread(book, proportion)  %>% 
  gather(book,proportion, `Sense & Sensibility`:`Northanger Abbey`)
  

#LEFT OFF MONDAY NEEDS RE-WORKED 
library(scales)
#ggplot(frequency, aes(x = proportion, y = Persuasion, color = abs(Persuasion - proportion))) +
#  geom_abline(color = "gray40", lty = 2) +
#  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
#  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
#  scale_x_log10(labels = percent_format()) +
#  scale_y_log10(labels = percent_format()) +
#  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
#  facet_wrap(~book, ncol = 2) # +
#  theme(legend.position="none") +
#  labs(y = "Persuasion", x = NULL)

```
 
##From Tidy Text to other formats
![From Tidy Text Format to Others](http://tidytextmining.com/images/tidyflow-ch-5.png)

#Sentiment Analysis

LEFT OFF ON SENTIMENT ANALYSIS CAN BE DONE AS AN INNER JOIN IN INTRO TAB
 *look at other sentiment tabs and see if anything to add then move to topic modeling
 *maybe insert my own example 
 
```{r}

#Three sentiment lexicons are in the tidytext package in the sentiments dataset. Let’s look at the words with a joy #score from the NRC lexicon. What are the most common joy words in Emma?

nrcjoy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

emma_joy <- tidy_books %>%
  filter(book == "Emma") %>%
  semi_join(nrcjoy) %>%
  count(word, sort = TRUE)

#noticed how we used the original tidy books dataset as opposed to to "cleaned_books_2" which we later made. Why? Stop words (i.e., "good", "young") are often overlapping with sentient phrases. 

#Or instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.

#LEFT OFF ON MONDAY 
 
```

##Jane Austen Novels: Example

#Topic Modelling
TOPIC MODELING (chps 5 and 6 with optimization for finding right number of topics in seperate script )


## Novels example

## Real World ExampleR

#More Advanced topics 

##Handy Function for Topic Visualization

##Finding the Optimal Number of Topics
We can use "unsupervised maching learning" algorthims to find the optimal number of topics
  *Take advantage of Parallel Processing in R



